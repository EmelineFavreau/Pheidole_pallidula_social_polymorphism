---
title: "Window read coverage"
author: "Emeline Favreau"
date: "24 July 2019"
output:
  html_document: default
  pdf_document: default
---

Using Bruniquel data for coding and non-coding regions, we ask whether each locus maps differently depending on the social type of the sample.
Here we compare the coverage of each base of the reference by social type.
There are 15 monogynous samples and 53 polygynous samples.

For all contigs, I calculated the mean coverage of 5kb windows for each sample. Briefly, for each sample, I subsetted the alignment file for just this contig (samtools view), I obtained the coverage for each window (bedtools coverageBed), I calculated the mean for each window (normalised by the median in this window, in R). I then concatenated each resulting table (for each sample, the table contains as many mean read coverage as 5kb window) and imported them here.

```{r load all the libraries, eval = TRUE, echo = FALSE, include = FALSE}
# get libraries
basic_libraries <- c("qqman", "ggplot2", "gridExtra", "ggrepel", "RColorBrewer", "tidyr", "reshape2", "plyr", "readr", "colorspace")
for (lib in basic_libraries) {
        if (require(package = lib, character.only = TRUE)) {
                print("Successful")
        } else {
                print("Installing")
                install.packages(lib)
                library(lib, character.only = TRUE )
        }
}

```


```{r import all the data, eval = TRUE, echo = FALSE, include = FALSE}
# set the directory where the data are
mydir <- "result/read_coverage_per_5kb_all_samples"
# list all the files in this directory
myfiles <- as.list(list.files(path = mydir, pattern = ".*all-samples-read-count5kb", full.names = TRUE))

# vector of contig names
contig_name_vec <- laply(myfiles, gsub, pattern = "result/read_coverage_per_5kb_all_samples/|_all-samples-read-count5kb", replacement = "")

# remove three problematic contigs (1st line are from bash loop, 2nd line from previous run of following R loop - very little coverage)
contig_name_vec <- contig_name_vec[!(contig_name_vec %in% c("contig_2271", "contig_2278", "contig_4048"))]

# import each dataset into a list
all_samples_read_count5kb_list <- llply(myfiles, read.table, fill = TRUE, header = TRUE)

# name list elements (ie contigs)
names(all_samples_read_count5kb_list) <- contig_name_vec

# import population info
pop_info_df <- read.csv(file = "S2_pheidole_pop_paper.csv")

# import name of contig with rank (M_enriched, not enriched, P_enriched)
bruniquel_contig_coverage_ordered <- read.table(file = "result/bruniquel_contig_coverage_ordered", header = TRUE)
```

To assess the difference of read coverage between monogynous and polygynous samples, each 5kb window of the genome is statistically assessed with a Wilcoxon test and a Kolmogorov-Smirnov test. 

For each sample, the contig is fragmented in 5kb window and read coverage is counted at each window. 
This window read coverage is normalised by the whole contig coverage average.


```{r wilcoxon and KS 5kb window, eval = TRUE, echo = FALSE, warning = FALSE}
# make a dataset list
dataset_list <- all_samples_read_count5kb_list

# run a wilcoxon test per window on those contigs
# run a KS test per window on those contigs

# create a test result dataframe
wilcoxon_ks_tests_df <- as.data.frame(matrix(ncol = 4))

# name columns
colnames(wilcoxon_ks_tests_df) <- c("window", "stat_test", "p_value", "contig")

# create vector for contigs that need to be troubleshot
rank_of_files_with_no_row <- c()
rank_of_files_with_superlow_coverage <- c()

# loop through contigs
for(this_contig_rank in 1:length(contig_name_vec)){
  # find the file (read depth per sample and per window)
  this_file <- dataset_list[[this_contig_rank]]
  
  # if this file has no row, make a note of it
  if(nrow(this_file) == 0){
    rank_of_files_with_no_row <- c(rank_of_files_with_no_row, this_contig_rank)
  } else {
      # some samples do not have read coverage for that 5kb window
      # replace all NA (added by read.table fill) by 0
      this_file[is.na(this_file)] <- 0
      
      # calculate the sum of coverage per sample
      sum_of_coverage_per_sample <- colSums(this_file)
      
      # checking for no coverage: subset to only the values that are unique
      unique_values <- unique(sum_of_coverage_per_sample)
      
      # checking for no coverage: calculate the mode of the sums
      mode_of_sums <- unique_values[which.max(tabulate(match(sum_of_coverage_per_sample, unique_values)))]
      
      # checking for no coverage: if the mode is 0
      if(mode_of_sums == 0){
        # checking for no coverage: save the contig rank for investigation
        rank_of_files_with_superlow_coverage <- c(rank_of_files_with_superlow_coverage, this_contig_rank)
    
      } else {
      
          # preparing for data wrangling: add rownames as window rank (1 is 0-5kb, 2 is 5-10kb, etc)
          row.names(this_file) <- seq(from = 1, to = nrow(this_file), by = 1)
          
          # preparing for data wrangling: add column names as samples (A01-P, etc)
          colnames(this_file) <- gsub(x = colnames(this_file), pattern = "\\.", replacement = "-")
          
          # preparing for data wrangling: vector of samples
          sample_vec <- colnames(this_file)
          
          # preparing for normalisation: calculate number of window in this contig
          num_of_windows <- nrow(this_file)
          
          # preparing for normalisation: obtain mean per sample in this contig
          this_file_mean_per_sample <- apply(this_file, 2, mean)
        
          # preparing for normalisation: make a result table with normalised read depth
          this_file_normalised <- this_file
        
          # normalise each read depth by mean (per sample)
          for(my_position in 1:length(sample_vec)){
            
            # subset window coverage file for one sample
            this_file_this_sample <- subset(this_file, select = colnames(this_file) == sample_vec[my_position])
            
            # normalise each window coverage by the sample mean
            this_file_this_sample_normalised <- this_file_this_sample / 
              this_file_mean_per_sample[names(this_file_mean_per_sample) == sample_vec[my_position]]
            
            # save result
            this_file_normalised[names(this_file_normalised) == sample_vec[my_position]] <- this_file_this_sample_normalised
          }
          
          # preparation for plotting: keep samples from each social type in a vector   
          mono_samples <- grep(pattern = "\\-M", x = colnames(this_file_normalised), value = TRUE)
          mono_samples <- c(mono_samples, "A56-N")
          poly_samples <- grep(pattern = "\\-P", x = colnames(this_file_normalised), value = TRUE) 
          poly_samples <- c(poly_samples, "I27-N", "muna-N", "andrea-N")
          
          # preparation for plotting: add window name
          this_file_normalised$window <- 1:num_of_windows
          
          # create two vectors for p-values of statistical test
          all_windows_wilcoxon_pvalue <- c()
          all_windows_KS_pvalue <- c()
          
          # loop through each window
          for(window_position in 1:num_of_windows){
             
             # combine normalised read depth for each type of samples
             M_samples_read_depth_vec <- unlist(subset(this_file_normalised, 
                                                       select = colnames(this_file_normalised) %in% mono_samples,
                                                       subset = window == window_position))
             P_samples_read_depth_vec <- unlist(subset(this_file_normalised,
                                                      select = colnames(this_file_normalised) %in% poly_samples,
                                                      subset = window == window_position))
              
             # run Wilcoxon test and store p-value
             all_windows_wilcoxon_pvalue <- c(all_windows_wilcoxon_pvalue, 
                                               wilcox.test(x = M_samples_read_depth_vec, P_samples_read_depth_vec)$p.value)
              
             # store p-value fromKS test 
             all_windows_KS_pvalue <- c(all_windows_KS_pvalue, 
                                               ks.test(x = M_samples_read_depth_vec, P_samples_read_depth_vec)$p.value)
            
          }
          
          # combine into a df for plotting
          read_depth_distribution_tests_df <- as.data.frame(cbind(1:num_of_windows,
                                                                  all_windows_wilcoxon_pvalue,
                                                                  all_windows_KS_pvalue))
          
          # name columns
          colnames(read_depth_distribution_tests_df) <- c("window", "wilcoxon", "ks")
        
          # transform the wide df into long df
          # make a window column (as a factor)
          read_depth_distribution_tests_df$window <- factor(read_depth_distribution_tests_df$window)
        
          # Specify id.vars: the variables to keep but not split apart on
          read_depth_distribution_tests_df_long <- melt(read_depth_distribution_tests_df, id.vars = c("window"))
        
          # rename columns
          colnames(read_depth_distribution_tests_df_long) <- c("window", "stat_test", "p_value")
          
          # update window name
          read_depth_distribution_tests_df_long$window <- paste(read_depth_distribution_tests_df_long$window,
                                                                contig_name_vec[this_contig_rank], sep = "")
          
          # add column for contig
          read_depth_distribution_tests_df_long$contig <- rep(contig_name_vec[this_contig_rank],
                                                              times = nrow(read_depth_distribution_tests_df_long))
          # update class
          read_depth_distribution_tests_df_long$stat_test <- as.character(read_depth_distribution_tests_df_long$stat_test)
          
          # store info in dataframe
          wilcoxon_ks_tests_df <- rbind(wilcoxon_ks_tests_df, read_depth_distribution_tests_df_long)
    }
  }
}

# troubleshoot those contigs:
# 3 contigs had no row in the original dataset
rank_of_files_with_no_row
# 40 contigs had very low coverage (in both social forms???)
rank_of_files_with_superlow_coverage

# remove NA 
wilcoxon_ks_tests_df <- wilcoxon_ks_tests_df[complete.cases(wilcoxon_ks_tests_df), ]

# adjust p value for multiple comparison in each test
wilcoxon_test_df <- subset(wilcoxon_ks_tests_df, subset = stat_test == "wilcoxon") 
wilcoxon_test_df$padj <- p.adjust(p = wilcoxon_test_df$p_value, method = "BH")

ks_test_df <- subset(wilcoxon_ks_tests_df, subset = stat_test == "ks") 
ks_test_df$padj <- p.adjust(p = ks_test_df$p_value, method = "BH")

# transform the data for plotting (-log10(p))
wilcoxon_test_df$padjlog10 <- -log10(wilcoxon_test_df$padj)
ks_test_df$padjlog10       <- -log10(ks_test_df$padj)

# set factor levels for window in the order of the contigs
ks_test_df$window <- factor(ks_test_df$window, levels = ks_test_df$window)

# set y axis label
xaxis_label <- "5kb window"

# change class
bruniquel_contig_coverage_ordered$contig <- as.character(bruniquel_contig_coverage_ordered$contig)

# merge the info about contig ranking and KS test
ks_test_df$contig_rank <- bruniquel_contig_coverage_ordered$contig_rank[match(ks_test_df$contig,
                                                                              bruniquel_contig_coverage_ordered$contig)]
# some contigs in this dataset is not in the bruniquel dataset (not sure why)
# remove them
ks_test_df <- ks_test_df[complete.cases(ks_test_df), ]

# sort dataframe by contig_rank and by window
ks_test_df_sorted <- ks_test_df[order(ks_test_df$contig_rank, ks_test_df$window), ]

# set number of contigs
num_contig <- length(unique(ks_test_df_sorted$contig))

# set the contig with the least fold change
neutral_contig <- bruniquel_contig_coverage_ordered[which.min(abs(bruniquel_contig_coverage_ordered$read_log2_fold_change - 0)),
                                                    "contig"]

# set a new column with annotation
ks_test_df_sorted$enrichment <- ifelse(ks_test_df_sorted$contig == neutral_contig,
                                             "neutral",
                                             "enriched")
# if the contig rank is smaller than the one of the neutral contig, the enrichment is in monogynous reads
ks_test_df_sorted$enrichment[ks_test_df_sorted$contig_rank < 
                               unique(unlist(subset(ks_test_df_sorted,
                                                    subset = enrichment == "neutral",
                                                    select = contig_rank)))] <- "enriched_in_M"

# if the contig is after the neutral contig, the enrichment is in polygynous reads
ks_test_df_sorted$enrichment[ks_test_df_sorted$contig_rank >
                               unique(unlist(subset(ks_test_df_sorted,
                                                    subset = enrichment == "neutral",
                                                    select = contig_rank)))] <- "enriched_in_P"

# set y axis label
yaxis_label <- "KS -log10(p)"

# plot Kolmogorov-Smirnov black-grey alternating
ggplot(ks_test_df_sorted,  aes(x = window, y = padjlog10, color = enrichment)) +
      geom_point(size = 0.25) +
      labs(x = xaxis_label, y = yaxis_label) +
      theme_classic() +
      theme(axis.text.x = element_blank(), legend.position = "none") +
      geom_hline(yintercept = -log10(0.05), color = "#018571") +
      scale_color_manual(values = rep(c("grey", "black"), num_contig)) +
      guides(col = guide_legend(ncol = 2)) +
      # Add highlighted points
      geom_point(data = subset(ks_test_df_sorted, enrichment == "neutral"), color = "#018571", size = 0.25)

# 2296 contigs
length(unique(ks_test_df_sorted$contig))

# change class
ks_test_df_sorted$test_rank <- 1:nrow(ks_test_df_sorted)

# rank of first window of neutral contig
rank_of_first_window_of_neutral_contig <- 
  unlist(subset(ks_test_df_sorted, subset = enrichment == "neutral", select = test_rank))[1]

# change class
ks_test_df_sorted$enrichment <- factor(ks_test_df_sorted$enrichment, levels = ks_test_df_sorted$enrichment)
ks_test_df_sorted$window <- factor(ks_test_df_sorted$window, levels = ks_test_df_sorted$window)
ks_test_df_sorted$contig_rank <- factor(ks_test_df_sorted$contig_rank, levels = ks_test_df_sorted$contig_rank)

# plot Kolmogorov-Smirnov gradient
ggplot(ks_test_df_sorted,  aes(x = window, y = padjlog10)) +
      geom_point(size = 0.5, aes(colour = contig_rank), alpha = 0.6, shape = 15) +
      labs(x = xaxis_label, y = yaxis_label) +
      theme(legend.position = "none") +
      geom_hline(yintercept = -log10(0.05), color = "grey95", size = 0.25) +
      scale_color_discrete_diverging(palette = "Blue-Yellow 3") +
      theme(panel.background = element_rect(fill = "grey48",
                                            colour = "black",
                                            size = 0.5,
                                            linetype = "solid"),
           panel.grid.major = element_blank(),
           panel.grid.minor = element_blank(),
           axis.ticks.y     = element_blank(),
           axis.text.y      = element_blank()) +
      # the left of the plot will have some space *0.01 unit and added 0.5 unit
      # same for the right of the plot
      scale_x_discrete(expand = expand_scale(mult = c(0.01, 0.01), add = c(0.5, 0.5))) +
      coord_flip()

# save for MS
ggsave(filename = "../figure3b_coverageplot.png", width = 4, height = 4, dpi = 600)
```
